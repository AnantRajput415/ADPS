{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Beta\n",
    "from torch.utils.data.sampler import BatchSampler, SubsetRandomSampler\n",
    "from utils import DrawLine\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "class Args:\n",
    "    gamma = 0.99\n",
    "    action_repeat = 8\n",
    "    img_stack = 4\n",
    "    seed = 0\n",
    "    render = False\n",
    "    vis = False\n",
    "    log_interval = 10\n",
    "\n",
    "args = Args()\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "torch.manual_seed(args.seed)\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "transition = np.dtype([('s', np.float64, (args.img_stack, 96, 96)), ('a', np.float64, (3,)), ('a_logp', np.float64),('r', np.float64), ('s_', np.float64, (args.img_stack, 96, 96))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "class Env:\n",
    "    \"\"\"\n",
    "    Environment wrapper for CarRacing\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, args):\n",
    "        self.env = gym.make('CarRacing-v3', render_mode=\"human\")\n",
    "        self.reward_threshold = self.env.spec.reward_threshold\n",
    "        self.args = args\n",
    "\n",
    "    def reset(self):\n",
    "        self.counter = 0\n",
    "        self.av_r = self.reward_memory()\n",
    "        self.die = False\n",
    "\n",
    "        obs, _ = self.env.reset()  # Handle tuple return\n",
    "        img_rgb = obs\n",
    "        img_gray = self.rgb2gray(img_rgb)\n",
    "        self.stack = [img_gray] * self.args.img_stack\n",
    "        return np.array(self.stack)\n",
    "\n",
    "    def step(self, action):\n",
    "        total_reward = 0\n",
    "        for _ in range(self.args.action_repeat):\n",
    "            img_rgb, reward, terminated, truncated, _ = self.env.step(action)\n",
    "            die = terminated or truncated\n",
    "\n",
    "            if die:\n",
    "                reward += 100\n",
    "            if np.mean(img_rgb[:, :, 1]) > 185.0:\n",
    "                reward -= 0.05\n",
    "\n",
    "            total_reward += reward\n",
    "            done = True if self.av_r(reward) <= -0.1 else False\n",
    "\n",
    "            if done or die:\n",
    "                break\n",
    "\n",
    "        img_gray = self.rgb2gray(img_rgb)\n",
    "        self.stack.pop(0)\n",
    "        self.stack.append(img_gray)\n",
    "        assert len(self.stack) == self.args.img_stack\n",
    "        return np.array(self.stack), total_reward, done, die\n",
    "\n",
    "    def render(self, *arg):\n",
    "        self.env.render(*arg)\n",
    "\n",
    "    @staticmethod\n",
    "    def rgb2gray(rgb, norm=True):\n",
    "        if isinstance(rgb, tuple):\n",
    "            rgb = np.array(rgb, dtype=np.float64)  # Ensure consistent dtype\n",
    "\n",
    "        if not (rgb.ndim == 3 and rgb.shape[2] == 3):\n",
    "            raise ValueError(\"Input must be an RGB image with shape (height, width, 3)\")\n",
    "\n",
    "        gray = np.dot(rgb[..., :3], [0.299, 0.587, 0.114])\n",
    "        if norm:\n",
    "            gray = gray / 128. - 1.\n",
    "        return gray\n",
    "\n",
    "    @staticmethod\n",
    "    def reward_memory():\n",
    "        count = 0\n",
    "        length = 100\n",
    "        history = np.zeros(length)\n",
    "\n",
    "        def memory(reward):\n",
    "            nonlocal count\n",
    "            history[count] = reward\n",
    "            count = (count + 1) % length\n",
    "            return np.mean(history)\n",
    "\n",
    "        return memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    Actor-Critic Network for PPO\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.cnn_base = nn.Sequential(  # input shape (4, 96, 96)\n",
    "            nn.Conv2d(args.img_stack, 8, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),  # activation\n",
    "            nn.Conv2d(8, 16, kernel_size=3, stride=2),  # (8, 47, 47)\n",
    "            nn.ReLU(),  # activation\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=2),  # (16, 23, 23)\n",
    "            nn.ReLU(),  # activation\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2),  # (32, 11, 11)\n",
    "            nn.ReLU(),  # activation\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1),  # (64, 5, 5)\n",
    "            nn.ReLU(),  # activation\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1),  # (128, 3, 3)\n",
    "            nn.ReLU(),  # activation\n",
    "        )  # output shape (256, 1, 1)\n",
    "        self.v = nn.Sequential(nn.Linear(256, 100), nn.ReLU(), nn.Linear(100, 1))\n",
    "        self.fc = nn.Sequential(nn.Linear(256, 100), nn.ReLU())\n",
    "        self.alpha_head = nn.Sequential(nn.Linear(100, 3), nn.Softplus())\n",
    "        self.beta_head = nn.Sequential(nn.Linear(100, 3), nn.Softplus())\n",
    "        self.apply(self._weights_init)\n",
    "\n",
    "    @staticmethod\n",
    "    def _weights_init(m):\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            nn.init.xavier_uniform_(m.weight, gain=nn.init.calculate_gain('relu'))\n",
    "            nn.init.constant_(m.bias, 0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn_base(x)\n",
    "        x = x.view(-1, 256)\n",
    "        v = self.v(x)\n",
    "        x = self.fc(x)\n",
    "        alpha = self.alpha_head(x) + 1\n",
    "        beta = self.beta_head(x) + 1\n",
    "\n",
    "        return (alpha, beta), v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \"\"\"\n",
    "    Agent for training\n",
    "    \"\"\"\n",
    "    max_grad_norm = 0.5\n",
    "    clip_param = 0.1  # epsilon in clipped loss\n",
    "    ppo_epoch = 10\n",
    "    buffer_capacity, batch_size = 2000, 128\n",
    "\n",
    "    def __init__(self):\n",
    "        self.training_step = 0\n",
    "        self.net = Net().double().to(device)\n",
    "        self.buffer = np.empty(self.buffer_capacity, dtype=transition)\n",
    "        self.counter = 0\n",
    "\n",
    "        self.optimizer = optim.Adam(self.net.parameters(), lr=1e-4)\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.from_numpy(state).double().to(device).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            alpha, beta = self.net(state)[0]\n",
    "        dist = Beta(alpha, beta)\n",
    "        action = dist.sample()\n",
    "        a_logp = dist.log_prob(action).sum(dim=1)\n",
    "\n",
    "        action = action.squeeze().cpu().numpy()\n",
    "        a_logp = a_logp.item()\n",
    "        return action, a_logp\n",
    "\n",
    "    def save_param(self):\n",
    "        torch.save(self.net.state_dict(), 'param/ppo_net_params1.pkl')\n",
    "\n",
    "    def store(self, transition):\n",
    "        self.buffer[self.counter] = transition\n",
    "        self.counter += 1\n",
    "        if self.counter == self.buffer_capacity:\n",
    "            self.counter = 0\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def load_param(self):\n",
    "        self.net.load_state_dict(torch.load('param/ppo_net_params1.pkl'))\n",
    "\n",
    "    def update(self):\n",
    "        self.training_step += 1\n",
    "\n",
    "        s = torch.tensor(self.buffer['s'], dtype=torch.double).to(device)\n",
    "        a = torch.tensor(self.buffer['a'], dtype=torch.double).to(device)\n",
    "        r = torch.tensor(self.buffer['r'], dtype=torch.double).to(device).view(-1, 1)\n",
    "        s_ = torch.tensor(self.buffer['s_'], dtype=torch.double).to(device)\n",
    "\n",
    "        old_a_logp = torch.tensor(self.buffer['a_logp'], dtype=torch.double).to(device).view(-1, 1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            target_v = r + args.gamma * self.net(s_)[1]\n",
    "            adv = target_v - self.net(s)[1]\n",
    "\n",
    "        for _ in range(self.ppo_epoch):\n",
    "            for index in BatchSampler(SubsetRandomSampler(range(self.buffer_capacity)), self.batch_size, False):\n",
    "\n",
    "                alpha, beta = self.net(s[index])[0]\n",
    "                dist = Beta(alpha, beta)\n",
    "                a_logp = dist.log_prob(a[index]).sum(dim=1, keepdim=True)\n",
    "                ratio = torch.exp(a_logp - old_a_logp[index])\n",
    "\n",
    "                surr1 = ratio * adv[index]\n",
    "                surr2 = torch.clamp(ratio, 1.0 - self.clip_param, 1.0 + self.clip_param) * adv[index]\n",
    "                action_loss = -torch.min(surr1, surr2).mean()\n",
    "                value_loss = F.smooth_l1_loss(self.net(s[index])[1], target_v[index])\n",
    "                loss = action_loss + 2. * value_loss\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 0\tLast score: 87.38\tMoving average score: 0.87\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1000\u001b[39m):\n\u001b[1;32m     17\u001b[0m     action, a_logp \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mselect_action(state)\n\u001b[0;32m---> 18\u001b[0m     state_, reward, done, die \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mrender:\n\u001b[1;32m     21\u001b[0m         env\u001b[38;5;241m.\u001b[39mrender()\n",
      "Cell \u001b[0;32mIn[3], line 28\u001b[0m, in \u001b[0;36mEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     26\u001b[0m total_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39maction_repeat):\n\u001b[0;32m---> 28\u001b[0m     img_rgb, reward, terminated, truncated, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     die \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m die:\n",
      "File \u001b[0;32m~/miniconda3/envs/ADPS/lib/python3.9/site-packages/gymnasium/wrappers/common.py:125\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[1;32m    114\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[ObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    115\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    123\u001b[0m \n\u001b[1;32m    124\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/miniconda3/envs/ADPS/lib/python3.9/site-packages/gymnasium/wrappers/common.py:393\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ADPS/lib/python3.9/site-packages/gymnasium/core.py:322\u001b[0m, in \u001b[0;36mWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[1;32m    320\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    321\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ADPS/lib/python3.9/site-packages/gymnasium/wrappers/common.py:285\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ADPS/lib/python3.9/site-packages/gymnasium/envs/box2d/car_racing.py:563\u001b[0m, in \u001b[0;36mCarRacing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    560\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworld\u001b[38;5;241m.\u001b[39mStep(\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m FPS, \u001b[38;5;241m6\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m30\u001b[39m, \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m30\u001b[39m)\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m FPS\n\u001b[0;32m--> 563\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_render\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstate_pixels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    565\u001b[0m step_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    566\u001b[0m terminated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ADPS/lib/python3.9/site-packages/gymnasium/envs/box2d/car_racing.py:640\u001b[0m, in \u001b[0;36mCarRacing._render\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msurf \u001b[38;5;241m=\u001b[39m pygame\u001b[38;5;241m.\u001b[39mtransform\u001b[38;5;241m.\u001b[39mflip(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msurf, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    639\u001b[0m \u001b[38;5;66;03m# showing stats\u001b[39;00m\n\u001b[0;32m--> 640\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_render_indicators\u001b[49m(WINDOW_W, WINDOW_H)\n\u001b[1;32m    642\u001b[0m font \u001b[38;5;241m=\u001b[39m pygame\u001b[38;5;241m.\u001b[39mfont\u001b[38;5;241m.\u001b[39mFont(pygame\u001b[38;5;241m.\u001b[39mfont\u001b[38;5;241m.\u001b[39mget_default_font(), \u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m    643\u001b[0m text \u001b[38;5;241m=\u001b[39m font\u001b[38;5;241m.\u001b[39mrender(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%04i\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreward, \u001b[38;5;28;01mTrue\u001b[39;00m, (\u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m255\u001b[39m), (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    agent = Agent()\n",
    "    env = Env(args)\n",
    "\n",
    "    training_records = []\n",
    "    scores = []\n",
    "    episodes = []\n",
    "    running_score = 0\n",
    "\n",
    "    for i_ep in range(100000):\n",
    "        score = 0\n",
    "        state = env.reset()\n",
    "\n",
    "        for t in range(1000):\n",
    "            action, a_logp = agent.select_action(state)\n",
    "            state_, reward, done, die = env.step(action * np.array([2., 1., 1.]) + np.array([-1., 0., 0.]))\n",
    "\n",
    "            if args.render:\n",
    "                env.render()\n",
    "            if agent.store((state, action, a_logp, reward, state_)):\n",
    "                agent.update()\n",
    "            score += reward\n",
    "            state = state_\n",
    "            if done or die:\n",
    "                break\n",
    "\n",
    "        running_score = running_score * 0.99 + score * 0.01\n",
    "\n",
    "        if i_ep % 2 == 0:\n",
    "            episodes.append(i_ep)\n",
    "            scores.append(running_score)\n",
    "            print('Ep {}\\tLast score: {:.2f}\\tMoving average score: {:.2f}'.format(i_ep, score, running_score))\n",
    "        if i_ep % 10 == 0:\n",
    "            agent.save_param()\n",
    "\n",
    "        if running_score > env.reward_threshold:\n",
    "            print(\"Solved! Running reward is now {} and the last episode runs to {}!\".format(running_score, score))\n",
    "            break\n",
    "\n",
    "    # Plot scores after training\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(episodes, scores, label=\"Moving Average Score\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Moving Average Score\")\n",
    "    plt.title(\"Training Progress\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.savefig(\"training_progress.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900\n"
     ]
    }
   ],
   "source": [
    "print(env.reward_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ADPS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
