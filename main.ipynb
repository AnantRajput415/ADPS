{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1+cu121\n",
      "12.1\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x77b24071bec0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from collections import deque, namedtuple\n",
    "import copy\n",
    "from swig import swig\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "# Enable autograd anomaly detection\n",
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium in ./adps/lib/python3.12/site-packages (1.0.0)\n",
      "Requirement already satisfied: numpy>=1.21.0 in ./adps/lib/python3.12/site-packages (from gymnasium) (2.1.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in ./adps/lib/python3.12/site-packages (from gymnasium) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in ./adps/lib/python3.12/site-packages (from gymnasium) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in ./adps/lib/python3.12/site-packages (from gymnasium) (0.0.4)\n",
      "Requirement already satisfied: gymnasium[accept-rom-license,atari] in ./adps/lib/python3.12/site-packages (1.0.0)\n",
      "\u001b[33mWARNING: gymnasium 1.0.0 does not provide the extra 'accept-rom-license'\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: numpy>=1.21.0 in ./adps/lib/python3.12/site-packages (from gymnasium[accept-rom-license,atari]) (2.1.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in ./adps/lib/python3.12/site-packages (from gymnasium[accept-rom-license,atari]) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in ./adps/lib/python3.12/site-packages (from gymnasium[accept-rom-license,atari]) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in ./adps/lib/python3.12/site-packages (from gymnasium[accept-rom-license,atari]) (0.0.4)\n",
      "Requirement already satisfied: ale-py>=0.9 in ./adps/lib/python3.12/site-packages (from gymnasium[accept-rom-license,atari]) (0.10.1)\n",
      "E: Could not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied)\n",
      "E: Unable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root?\n",
      "Requirement already satisfied: gymnasium[box2d] in ./adps/lib/python3.12/site-packages (1.0.0)\n",
      "Requirement already satisfied: numpy>=1.21.0 in ./adps/lib/python3.12/site-packages (from gymnasium[box2d]) (2.1.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in ./adps/lib/python3.12/site-packages (from gymnasium[box2d]) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in ./adps/lib/python3.12/site-packages (from gymnasium[box2d]) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in ./adps/lib/python3.12/site-packages (from gymnasium[box2d]) (0.0.4)\n",
      "Requirement already satisfied: box2d-py==2.3.5 in ./adps/lib/python3.12/site-packages (from gymnasium[box2d]) (2.3.5)\n",
      "Requirement already satisfied: pygame>=2.1.3 in ./adps/lib/python3.12/site-packages (from gymnasium[box2d]) (2.6.1)\n",
      "Requirement already satisfied: swig==4.* in ./adps/lib/python3.12/site-packages (from gymnasium[box2d]) (4.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install gymnasium\n",
    "!pip install \"gymnasium[atari, accept-rom-license]\"\n",
    "!apt-get install -y swig\n",
    "!pip install gymnasium[box2d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, input_shape, action_size):\n",
    "        super(Actor, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_shape[2], 16, kernel_size=4, stride=2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=2)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=3, stride=1)\n",
    "\n",
    "        conv_out_size = self._get_conv_output(input_shape)\n",
    "\n",
    "        self.fc1 = nn.Linear(conv_out_size, 256)\n",
    "        self.fc2 = nn.Linear(256, action_size)\n",
    "\n",
    "    def _get_conv_output(self, shape):\n",
    "        input = torch.rand(1, *shape)\n",
    "        input = input.permute(0, 3, 1, 2)\n",
    "        output = self.conv3(self.conv2(self.conv1(input)))\n",
    "        return int(np.prod(output.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() == 3:\n",
    "            x = x.unsqueeze(0)\n",
    "        if x.size(1) != 3:\n",
    "            x = x.permute(0, 3, 1, 2)\n",
    "        \n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        action = torch.tanh(self.fc2(x))  # Constrain output between -1 and 1\n",
    "        return action\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, input_shape, action_size):\n",
    "        super(Critic, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_shape[2], 16, kernel_size=4, stride=2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=2)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=3, stride=1)\n",
    "\n",
    "        conv_out_size = self._get_conv_output(input_shape)\n",
    "\n",
    "        self.fc1 = nn.Linear(conv_out_size + action_size, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, 1)\n",
    "\n",
    "    def _get_conv_output(self, shape):\n",
    "        input = torch.rand(1, *shape)\n",
    "        input = input.permute(0, 3, 1, 2)\n",
    "        output = self.conv3(self.conv2(self.conv1(input)))\n",
    "        return int(np.prod(output.size()))\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        if state.dim() == 3:\n",
    "            state = state.unsqueeze(0)\n",
    "        if state.size(1) != 3:\n",
    "            state = state.permute(0, 3, 1, 2)\n",
    "        \n",
    "        x = F.relu(self.conv1(state))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = torch.cat([x, action], dim=1)  # Concatenate state and action\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        Q_value = self.fc3(x)\n",
    "        return Q_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State shape:  (96, 96, 3)\n",
      "state size:  96\n",
      "action size:  3\n",
      "action space: Box([-1.  0.  0.], 1.0, (3,), float32)\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make(\"CarRacing-v3\")\n",
    "state_shape = env.observation_space.shape\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.shape[0]\n",
    "action_space = env.action_space\n",
    "print(\"State shape: \", state_shape)\n",
    "print(\"state size: \", state_size)\n",
    "print(\"action size: \", action_size)\n",
    "print(\"action space:\", action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "minibatch_size = 128\n",
    "discount_factor = 0.99\n",
    "replay_buffer_size = int(1e5)\n",
    "tau = 1e-3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    def __init__(self, capacity, state_shape, action_dim, device):\n",
    "        self.capacity = capacity\n",
    "        self.device = device\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "        self.state_shape = state_shape\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.memory, batch_size)\n",
    "        \n",
    "        # Pre-allocate numpy arrays\n",
    "        states = np.zeros((batch_size, *self.state_shape), dtype=np.float32)\n",
    "        actions = np.zeros((batch_size, self.action_dim), dtype=np.float32)\n",
    "        rewards = np.zeros((batch_size, 1), dtype=np.float32)\n",
    "        next_states = np.zeros((batch_size, *self.state_shape), dtype=np.float32)\n",
    "        dones = np.zeros((batch_size, 1), dtype=np.float32)\n",
    "\n",
    "        # Fill the numpy arrays\n",
    "        for i, (state, action, reward, next_state, done) in enumerate(batch):\n",
    "            states[i] = state\n",
    "            actions[i] = action\n",
    "            rewards[i] = reward\n",
    "            next_states[i] = next_state\n",
    "            dones[i] = done\n",
    "\n",
    "        # Convert numpy arrays to tensors\n",
    "        return (\n",
    "            torch.from_numpy(states).pin_memory().to(self.device,non_blocking=True),\n",
    "            torch.from_numpy(actions).to(self.device),\n",
    "            torch.from_numpy(rewards).to(self.device),\n",
    "            torch.from_numpy(next_states).to(self.device),\n",
    "            torch.from_numpy(dones).to(self.device)\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUNoise:\n",
    "    def __init__(self, size, mu=0., theta=0.15, sigma=0.2):\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.random.randn(len(x))\n",
    "        self.state = x + dx\n",
    "        return self.state.tolist()  # Return as a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, state_shape, action_size, actor_lr=1e-4, critic_lr=1e-3, replay_buffer_size=10000, minibatch_size=32):\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.state_shape = state_shape\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # Actor network\n",
    "        self.actor = Actor(state_shape, action_size).to(self.device)\n",
    "        self.target_actor = Actor(state_shape, action_size).to(self.device)\n",
    "        self.target_actor.load_state_dict(self.actor.state_dict())\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
    "\n",
    "        # Critic network\n",
    "        self.critic = Critic(state_shape, action_size).to(self.device)\n",
    "        self.target_critic = Critic(state_shape, action_size).to(self.device)\n",
    "        self.target_critic.load_state_dict(self.critic.state_dict())\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=critic_lr)\n",
    "\n",
    "        # Replay buffer\n",
    "        self.memory = ReplayMemory(replay_buffer_size, state_shape, action_size, self.device)\n",
    "        self.minibatch_size = minibatch_size\n",
    "\n",
    "    def act(self, state, noise_scale=0.1):\n",
    "        \"\"\"Get action using the actor network and add noise for exploration.\"\"\"\n",
    "        self.actor.eval()\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n",
    "            action = self.actor(state_tensor).cpu().data.numpy().squeeze()\n",
    "        self.actor.train()\n",
    "\n",
    "        # Add noise for exploration\n",
    "        noise = noise_scale * np.random.randn(self.action_size)\n",
    "        action = action + noise\n",
    "        return np.clip(action, -1, 1)\n",
    "\n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Save experience to replay memory and learn if enough samples.\"\"\"\n",
    "        self.memory.push(state, action, reward, next_state, done)\n",
    "        if len(self.memory) > self.minibatch_size:\n",
    "            experiences = self.memory.sample(self.minibatch_size)\n",
    "            self.learn(experiences, 0.99)\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update actor and critic networks from sampled experiences.\"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # Critic update: Q-value estimation\n",
    "        next_actions = self.target_actor(next_states)\n",
    "        Q_targets_next = self.target_critic(next_states, next_actions).detach()\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "\n",
    "        Q_expected = self.critic(states, actions)\n",
    "        critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        \n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # Actor update: Maximize Q-value predicted by the critic\n",
    "        actions_pred = self.actor(states)\n",
    "        actor_loss = -self.critic(states, actions_pred).mean()\n",
    "        \n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # Update target networks\n",
    "        self.soft_update(self.critic, self.target_critic, 1e-3)\n",
    "        self.soft_update(self.actor, self.target_actor, 1e-3)\n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(state_shape, action_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 2/2000 [01:36<26:54:44, 48.49s/it]"
     ]
    }
   ],
   "source": [
    "def train_agent(agent, env, number_episodes=2000, max_timesteps_per_episode=1000, target_score=200.0):\n",
    "    scores_window = deque(maxlen=100)\n",
    "    \n",
    "    noise_stddev = 0.2  # Standard deviation for noise\n",
    "    noise_decay = 0.995  # Noise decay over episodes\n",
    "    noise_min = 0.01  # Minimum noise level for exploration\n",
    "\n",
    "    for episode in tqdm(range(1, number_episodes + 1), desc=\"Training Progress\"):\n",
    "        state, _ = env.reset()\n",
    "        score = 0\n",
    "        \n",
    "        noise = np.random.normal(0, noise_stddev, agent.action_size)\n",
    "        \n",
    "        for t in range(max_timesteps_per_episode):\n",
    "            action = agent.act(state)  # Get action from the policy (Actor network)\n",
    "            action = action + noise  # Add exploration noise\n",
    "            action = np.clip(action, -1, 1)  # Clip action to be within the valid range\n",
    "            \n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            \n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            \n",
    "            state = next_state\n",
    "            score += reward\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        scores_window.append(score)\n",
    "        noise_stddev = max(noise_min, noise_stddev * noise_decay)  # Decay the noise\n",
    "        \n",
    "        if episode % 100 == 0:\n",
    "            avg_score = np.mean(scores_window)\n",
    "            print(f'\\rEpisode {episode}\\tAverage Score: {avg_score:.2f}\\tNoise StdDev: {noise_stddev:.2f}')\n",
    "            torch.save(agent.actor.state_dict(), f'checkpoint_actor.pth')\n",
    "            torch.save(agent.critic.state_dict(), f'checkpoint_critic.pth')\n",
    "        \n",
    "        if np.mean(scores_window) >= target_score:\n",
    "            print(f'\\nEnvironment solved in {episode-100:d} episodes!\\tAverage Score: {np.mean(scores_window):.2f}')\n",
    "            torch.save(agent.actor.state_dict(), 'checkpoint_actor_final.pth')\n",
    "            torch.save(agent.critic.state_dict(), 'checkpoint_critic_final.pth')\n",
    "            return True\n",
    "    \n",
    "    print(f'\\nEnvironment not solved. Final Average Score: {np.mean(scores_window):.2f}')\n",
    "    torch.save(agent.actor.state_dict(), 'checkpoint_actor_final_us.pth')\n",
    "    torch.save(agent.critic.state_dict(), 'checkpoint_critic_final_us.pth')\n",
    "    return False\n",
    "\n",
    "# Usage\n",
    "train_agent(agent, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
