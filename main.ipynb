{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from collections import deque, namedtuple\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Brain(nn.Module):\n",
    "    def __init__(self, input_shape, output_size):\n",
    "        super(Brain, self).__init__()\n",
    "        self.input_shape = input_shape\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(input_shape[2], 32, kernel_size=4, stride=2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "\n",
    "        # Calculate the size of the output from the last conv layer\n",
    "        conv_out_size = self._get_conv_output(input_shape)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(conv_out_size, 512)\n",
    "        self.fc2 = nn.Linear(512, output_size)\n",
    "\n",
    "    def _get_conv_output(self, shape):\n",
    "        input = torch.rand(1, *shape)\n",
    "        input = input.permute(0, 3, 1, 2)\n",
    "        output = self.conv3(self.conv2(self.conv1(input)))\n",
    "        return int(np.prod(output.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() == 3:\n",
    "            x = x.unsqueeze(0)\n",
    "        if x.size(1) != self.input_shape[2]:\n",
    "            x = x.permute(0, 3, 1, 2)\n",
    "        \n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State shape:  (96, 96, 3)\n",
      "state size:  96\n",
      "action size:  3\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make(\"CarRacing-v2\")\n",
    "state_shape = env.observation_space.shape\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.shape[0]\n",
    "print(\"State shape: \", state_shape)\n",
    "print(\"state size: \", state_size)\n",
    "print(\"action size: \", action_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "minibatch_size = 64 \n",
    "discount_factor = 0.99\n",
    "replay_buffer_size = int(1e5)\n",
    "tau = 1e-3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    def __init__(self, capacity, device=\"cuda:0\" if torch.cuda.is_available() else \"cpu\"):\n",
    "        self.device = torch.device(device)\n",
    "        self.capacity = capacity\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        # Ensure state and next_state are numpy arrays\n",
    "        state = np.array(state)\n",
    "        next_state = np.array(next_state)\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        experiences = random.sample(self.memory, k=batch_size)\n",
    "        \n",
    "        states = torch.from_numpy(np.array([e[0] for e in experiences if e is not None])).float().to(self.device)\n",
    "        actions = torch.from_numpy(np.array([e[1] for e in experiences if e is not None])).float().to(self.device)\n",
    "        rewards = torch.from_numpy(np.array([e[2] for e in experiences if e is not None])).float().unsqueeze(1).to(self.device)\n",
    "        next_states = torch.from_numpy(np.array([e[3] for e in experiences if e is not None])).float().to(self.device)\n",
    "        dones = torch.from_numpy(np.array([e[4] for e in experiences if e is not None]).astype(np.uint8)).float().unsqueeze(1).to(self.device)\n",
    "        \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUNoise:\n",
    "    def __init__(self, size, mu=0., theta=0.15, sigma=0.2):\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.random.randn(len(x))\n",
    "        self.state = x + dx\n",
    "        return self.state.tolist()  # Return as a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "UPDATE_EVERY = 4\n",
    "\n",
    "class Agent():\n",
    "    def __init__(self, state_shape, action_size, learning_rate=1e-4, replay_buffer_size=100000, minibatch_size=64, tau=1e-3):\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.state_shape = state_shape\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        # Using actor-critic approach for continuous action space\n",
    "        self.actor = Brain(state_shape, action_size).to(self.device)\n",
    "        self.critic = Brain(state_shape, 1).to(self.device)  # Critic outputs a single value\n",
    "        self.target_actor = Brain(state_shape, action_size).to(self.device)\n",
    "        self.target_critic = Brain(state_shape, 1).to(self.device)\n",
    "        \n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=learning_rate)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=learning_rate)\n",
    "        \n",
    "        self.memory = ReplayMemory(replay_buffer_size, self.device)\n",
    "        self.t_step = 0\n",
    "        \n",
    "        self.noise = OUNoise(action_size)\n",
    "        \n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.tau = tau\n",
    "\n",
    "    def act(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n",
    "        self.actor.eval()\n",
    "        with torch.no_grad():\n",
    "            action = self.actor(state).cpu().numpy().squeeze()\n",
    "        self.actor.train()\n",
    "        action += self.noise.sample()\n",
    "        \n",
    "        # Clip actions between -1 and 1, then convert to list\n",
    "        action = np.clip(action, -1, 1).tolist()\n",
    "        \n",
    "        # Ensure the correct action format for CarRacing-v2\n",
    "        steering = action[0]\n",
    "        gas = max(0, action[1])\n",
    "        brake = max(0, -action[1])\n",
    "        \n",
    "        return [steering, gas, brake]\n",
    "\n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        # Save experience in replay memory\n",
    "        self.memory.push(state, action, reward, next_state, done)\n",
    "        \n",
    "        # Learn every UPDATE_EVERY time steps.\n",
    "        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
    "        if self.t_step == 0:\n",
    "            # If enough samples are available in memory, get random subset and learn\n",
    "            if len(self.memory) > self.minibatch_size:\n",
    "                experiences = self.memory.sample(self.minibatch_size)\n",
    "                self.learn(experiences, 0.99)  # Using 0.99 as discount factor\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # Update critic\n",
    "        actions_next = self.target_actor(next_states)\n",
    "        Q_targets_next = self.target_critic(next_states)\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "        Q_expected = self.critic(states)\n",
    "        critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        \n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # Update actor\n",
    "        actions_pred = self.actor(states)\n",
    "        actor_loss = -self.critic(states).mean()\n",
    "        \n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # Update target networks\n",
    "        self.soft_update(self.critic, self.target_critic, self.tau)\n",
    "        self.soft_update(self.actor, self.target_actor, self.tau)\n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(state_shape, action_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[83], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m next_state, reward, done, _, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     26\u001b[0m next_state \u001b[38;5;241m=\u001b[39m normalize_state(next_state)\n\u001b[0;32m---> 27\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[1;32m     29\u001b[0m score \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "Cell \u001b[0;32mIn[71], line 53\u001b[0m, in \u001b[0;36mAgent.step\u001b[0;34m(self, state, action, reward, next_state, done)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt_step \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;66;03m# If enough samples are available in memory, get random subset and learn\u001b[39;00m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory) \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mminibatch_size:\n\u001b[0;32m---> 53\u001b[0m         experiences \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmemory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mminibatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearn(experiences, \u001b[38;5;241m0.99\u001b[39m)\n",
      "Cell \u001b[0;32mIn[48], line 16\u001b[0m, in \u001b[0;36mReplayMemory.sample\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msample\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch_size):\n\u001b[1;32m     14\u001b[0m     experiences \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory, k\u001b[38;5;241m=\u001b[39mbatch_size)\n\u001b[0;32m---> 16\u001b[0m     states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43me\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mexperiences\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     17\u001b[0m     actions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39marray([e[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m experiences \u001b[38;5;28;01mif\u001b[39;00m e \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]))\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     18\u001b[0m     rewards \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39marray([e[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m experiences \u001b[38;5;28;01mif\u001b[39;00m e \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]))\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "number_episodes = 2000\n",
    "max_timesteps_per_episode = 1000\n",
    "scores_window = deque(maxlen=100)\n",
    "\n",
    "def normalize_state(state):\n",
    "    return (state - state.mean()) / (state.std()+ 1e-8)\n",
    "\n",
    "epsilon = 1.0\n",
    "epsilon_decay = 0.995\n",
    "epsilon_min = 0.01\n",
    "\n",
    "for episode in range(1, number_episodes + 1):\n",
    "    state, _ = env.reset()\n",
    "    state = normalize_state(state)\n",
    "    agent.noise.reset()\n",
    "    score = 0\n",
    "    \n",
    "    \n",
    "    for t in range(max_timesteps_per_episode):\n",
    "        if np.random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = agent.act(state)\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        next_state = normalize_state(next_state)\n",
    "        agent.step(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        score += reward\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    scores_window.append(score)\n",
    "    \n",
    "    # print(f'\\rEpisode {episode}\\tAverage Score: {np.mean(scores_window):.2f}')\n",
    "    if episode % 100 == 0:\n",
    "        print(f'\\rEpisode {episode}\\tAverage Score: {np.mean(scores_window):.2f}')\n",
    "        torch.save(agent.actor.state_dict(), f'checkpoint_actor_{episode}.pth')\n",
    "        torch.save(agent.critic.state_dict(), f'checkpoint_critic_{episode}.pth')\n",
    "    \n",
    "    if np.mean(scores_window) >= 200.0:\n",
    "        print(f'\\nEnvironment solved in {episode-100:d} episodes!\\tAverage Score: {np.mean(scores_window):.2f}')\n",
    "        torch.save(agent.actor.state_dict(), 'checkpoint_actor.pth')\n",
    "        torch.save(agent.critic.state_dict(), 'checkpoint_critic.pth')\n",
    "        break\n",
    "    \n",
    "if np.mean(scores_window) < 200.0:\n",
    "    print(f'\\nEnvironment not solved. Final Average Score: {np.mean(scores_window):.2f}')\n",
    "    torch.save(agent.actor.state_dict(), 'checkpoint_actor_final.pth')\n",
    "    torch.save(agent.critic.state_dict(), 'checkpoint_critic_final.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "in method 'b2RevoluteJoint___SetMotorSpeed', argument 2 of type 'float32'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[80], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(action, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m     24\u001b[0m     action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(action, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m---> 26\u001b[0m next_state, reward, done, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m    \u001b[38;5;66;03m# Normalize and convert types\u001b[39;00m\n\u001b[1;32m     28\u001b[0m next_state \u001b[38;5;241m=\u001b[39m next_state\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(next_state, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;28;01melse\u001b[39;00m next_state\n",
      "File \u001b[0;32m~/Documents/work/ADPS/adps/lib/python3.12/site-packages/gymnasium/wrappers/time_limit.py:57\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m     47\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m \n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/Documents/work/ADPS/adps/lib/python3.12/site-packages/gymnasium/wrappers/order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/work/ADPS/adps/lib/python3.12/site-packages/gymnasium/wrappers/env_checker.py:51\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/work/ADPS/adps/lib/python3.12/site-packages/gymnasium/envs/box2d/car_racing.py:549\u001b[0m, in \u001b[0;36mCarRacing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcar\u001b[38;5;241m.\u001b[39mgas(\u001b[38;5;241m0.2\u001b[39m \u001b[38;5;241m*\u001b[39m (action \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m))\n\u001b[1;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcar\u001b[38;5;241m.\u001b[39mbrake(\u001b[38;5;241m0.8\u001b[39m \u001b[38;5;241m*\u001b[39m (action \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m))\n\u001b[0;32m--> 549\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mFPS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworld\u001b[38;5;241m.\u001b[39mStep(\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m FPS, \u001b[38;5;241m6\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m30\u001b[39m, \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m30\u001b[39m)\n\u001b[1;32m    551\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m FPS\n",
      "File \u001b[0;32m~/Documents/work/ADPS/adps/lib/python3.12/site-packages/gymnasium/envs/box2d/car_dynamics.py:177\u001b[0m, in \u001b[0;36mCar.step\u001b[0;34m(self, dt)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28mdir\u001b[39m \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msign(w\u001b[38;5;241m.\u001b[39msteer \u001b[38;5;241m-\u001b[39m w\u001b[38;5;241m.\u001b[39mjoint\u001b[38;5;241m.\u001b[39mangle)\n\u001b[1;32m    176\u001b[0m val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mabs\u001b[39m(w\u001b[38;5;241m.\u001b[39msteer \u001b[38;5;241m-\u001b[39m w\u001b[38;5;241m.\u001b[39mjoint\u001b[38;5;241m.\u001b[39mangle)\n\u001b[0;32m--> 177\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmotorSpeed\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdir\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;241m50.0\u001b[39m \u001b[38;5;241m*\u001b[39m val, \u001b[38;5;241m3.0\u001b[39m)\n\u001b[1;32m    179\u001b[0m \u001b[38;5;66;03m# Position => friction_limit\u001b[39;00m\n\u001b[1;32m    180\u001b[0m grass \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: in method 'b2RevoluteJoint___SetMotorSpeed', argument 2 of type 'float32'"
     ]
    }
   ],
   "source": [
    "def normalize_state(state):\n",
    "    return (state - state.mean()) / (state.std() + 1e-8).astype(np.float32)\n",
    "\n",
    "number_episodes = 5000  # Increased from 2000\n",
    "max_timesteps_per_episode = 1000\n",
    "scores_window = deque(maxlen=100)\n",
    "\n",
    "epsilon = 1.0\n",
    "epsilon_decay = 0.995\n",
    "epsilon_min = 0.01\n",
    "\n",
    "for episode in range(1, number_episodes + 1):\n",
    "    state, _ = env.reset()\n",
    "    state = normalize_state(state).astype(np.float32)\n",
    "    agent.noise.reset()\n",
    "    score = 0\n",
    "    \n",
    "    for t in range(max_timesteps_per_episode):\n",
    "        if np.random.random() < epsilon:\n",
    "            action = env.action_space\n",
    "            \n",
    "        else:\n",
    "            action = agent.act(state)\n",
    "        if isinstance(action, list):\n",
    "            action = np.array(action, dtype=np.float32)\n",
    "            \n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "           # Normalize and convert types\n",
    "        next_state = next_state.astype(np.float32) if isinstance(next_state, np.ndarray) else next_state\n",
    "        \n",
    "        reward = np.float32(reward)\n",
    "        agent.step(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        score += reward\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    scores_window.append(score)\n",
    "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "    \n",
    "    # print(f'\\rEpisode {episode}\\tAverage Score: {np.mean(scores_window):.2f}\\tEpsilon: {epsilon:.2f}')\n",
    "    if episode % 100 == 0:\n",
    "        print(f'\\rEpisode {episode}\\tAverage Score: {np.mean(scores_window):.2f}\\tEpsilon: {epsilon:.2f}')\n",
    "        # Save checkpoints periodically\n",
    "        torch.save(agent.actor.state_dict(), f'checkpoint_actor_{episode}.pth')\n",
    "        torch.save(agent.critic.state_dict(), f'checkpoint_critic_{episode}.pth')\n",
    "    \n",
    "    if np.mean(scores_window) >= 200.0:\n",
    "        print(f'\\nEnvironment solved in {episode-100:d} episodes!\\tAverage Score: {np.mean(scores_window):.2f}')\n",
    "        torch.save(agent.actor.state_dict(), 'checkpoint_actor_final.pth')\n",
    "        torch.save(agent.critic.state_dict(), 'checkpoint_critic_final.pth')\n",
    "        break\n",
    "\n",
    "# If not solved, save the final model anyway\n",
    "if np.mean(scores_window) < 200.0:\n",
    "    print(f'\\nEnvironment not solved. Final Average Score: {np.mean(scores_window):.2f}')\n",
    "    torch.save(agent.actor.state_dict(), 'checkpoint_actor_final.pth')\n",
    "    torch.save(agent.critic.state_dict(), 'checkpoint_critic_final.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import io\n",
    "import base64\n",
    "import imageio\n",
    "from IPython.display import HTML, display\n",
    "from gymnasium.wrappers.monitoring.video_recorder import VideoRecorder\n",
    "\n",
    "def show_video_of_model(agent, env_name):\n",
    "    env = gym.make(env_name, render_mode='rgb_array')\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    frames = []\n",
    "    while not done:\n",
    "        frame = env.render()\n",
    "        frames.append(frame)\n",
    "        action = agent.act(state)\n",
    "        state, reward, done, _, _ = env.step(action.item())\n",
    "    env.close()\n",
    "    imageio.mimsave('video.mp4', frames, fps=30)\n",
    "\n",
    "show_video_of_model(agent, 'LunarLander-v2')\n",
    "\n",
    "def show_video():\n",
    "    mp4list = glob.glob('*.mp4')\n",
    "    if len(mp4list) > 0:\n",
    "        mp4 = mp4list[0]\n",
    "        video = io.open(mp4, 'r+b').read()\n",
    "        encoded = base64.b64encode(video)\n",
    "        display(HTML(data='''<video alt=\"test\" autoplay\n",
    "                loop controls style=\"height: 400px;\">\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii'))))\n",
    "    else:\n",
    "        print(\"Could not find video\")\n",
    "\n",
    "show_video()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
