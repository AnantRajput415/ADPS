{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from collections import deque, namedtuple\n",
    "import copy\n",
    "from swig import swig\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gymnasium\n",
    "!pip install \"gymnasium[atari, accept-rom-license]\"\n",
    "!apt-get install -y swig\n",
    "!pip install gymnasium[box2d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Brain(nn.Module):\n",
    "    def __init__(self, input_shape, output_size):\n",
    "        super(Brain, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_shape[2], 16, kernel_size=4, stride=2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=2)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=3, stride=1)\n",
    "\n",
    "        conv_out_size = self._get_conv_output(input_shape)\n",
    "\n",
    "        self.fc1 = nn.Linear(conv_out_size, 256)\n",
    "        self.fc2 = nn.Linear(256, output_size)\n",
    "\n",
    "    def _get_conv_output(self, shape):\n",
    "        input = torch.rand(1, *shape)\n",
    "        input = input.permute(0, 3, 1, 2)\n",
    "        output = self.conv3(self.conv2(self.conv1(input)))\n",
    "        return int(np.prod(output.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() == 3:\n",
    "            x = x.unsqueeze(0)\n",
    "        if x.size(1) != 3:\n",
    "            x = x.permute(0, 3, 1, 2)\n",
    "        \n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "DependencyNotInstalled",
     "evalue": "Box2D is not installed, you can install it by run `pip install swig` followed by `pip install \"gymnasium[box2d]\"`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/work/ADPS/adps/lib/python3.12/site-packages/gymnasium/envs/box2d/bipedal_walker.py:15\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 15\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mBox2D\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mBox2D\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mb2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     17\u001b[0m         circleShape,\n\u001b[1;32m     18\u001b[0m         contactListener,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m         revoluteJointDef,\n\u001b[1;32m     23\u001b[0m     )\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'Box2D'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mDependencyNotInstalled\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgymnasium\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mgym\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCarRacing-v3\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m state_shape \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mobservation_space\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m      4\u001b[0m state_size \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mobservation_space\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/work/ADPS/adps/lib/python3.12/site-packages/gymnasium/envs/registration.py:702\u001b[0m, in \u001b[0;36mmake\u001b[0;34m(id, max_episode_steps, disable_env_checker, **kwargs)\u001b[0m\n\u001b[1;32m    699\u001b[0m     env_creator \u001b[38;5;241m=\u001b[39m env_spec\u001b[38;5;241m.\u001b[39mentry_point\n\u001b[1;32m    700\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    701\u001b[0m     \u001b[38;5;66;03m# Assume it's a string\u001b[39;00m\n\u001b[0;32m--> 702\u001b[0m     env_creator \u001b[38;5;241m=\u001b[39m \u001b[43mload_env_creator\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv_spec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mentry_point\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;66;03m# Determine if to use the rendering\u001b[39;00m\n\u001b[1;32m    705\u001b[0m render_modes: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/work/ADPS/adps/lib/python3.12/site-packages/gymnasium/envs/registration.py:549\u001b[0m, in \u001b[0;36mload_env_creator\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Loads an environment with name of style ``\"(import path):(environment name)\"`` and returns the environment creation function, normally the environment class type.\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \n\u001b[1;32m    542\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[38;5;124;03m    The environment constructor for the given environment name.\u001b[39;00m\n\u001b[1;32m    547\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    548\u001b[0m mod_name, attr_name \u001b[38;5;241m=\u001b[39m name\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 549\u001b[0m mod \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    550\u001b[0m fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(mod, attr_name)\n\u001b[1;32m    551\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/importlib/__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1387\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1310\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:488\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1387\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1331\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:935\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:995\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:488\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m~/work/ADPS/adps/lib/python3.12/site-packages/gymnasium/envs/box2d/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgymnasium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbox2d\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbipedal_walker\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BipedalWalker, BipedalWalkerHardcore\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgymnasium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbox2d\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcar_racing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CarRacing\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgymnasium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbox2d\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlunar_lander\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LunarLander, LunarLanderContinuous\n",
      "File \u001b[0;32m~/work/ADPS/adps/lib/python3.12/site-packages/gymnasium/envs/box2d/bipedal_walker.py:25\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mBox2D\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mb2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     17\u001b[0m         circleShape,\n\u001b[1;32m     18\u001b[0m         contactListener,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m         revoluteJointDef,\n\u001b[1;32m     23\u001b[0m     )\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 25\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DependencyNotInstalled(\n\u001b[1;32m     26\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBox2D is not installed, you can install it by run `pip install swig` followed by `pip install \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgymnasium[box2d]\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     27\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpygame\u001b[39;00m\n",
      "\u001b[0;31mDependencyNotInstalled\u001b[0m: Box2D is not installed, you can install it by run `pip install swig` followed by `pip install \"gymnasium[box2d]\"`"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make(\"CarRacing-v2\")\n",
    "state_shape = env.observation_space.shape\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.shape[0]\n",
    "print(\"State shape: \", state_shape)\n",
    "print(\"state size: \", state_size)\n",
    "print(\"action size: \", action_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "minibatch_size = 64 \n",
    "discount_factor = 0.99\n",
    "replay_buffer_size = int(1e5)\n",
    "tau = 1e-3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    def __init__(self, capacity, state_shape, action_dim, device):\n",
    "        self.capacity = capacity\n",
    "        self.device = device\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "        self.state_shape = state_shape\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.memory, batch_size)\n",
    "        \n",
    "        # Pre-allocate numpy arrays\n",
    "        states = np.zeros((batch_size, *self.state_shape), dtype=np.float32)\n",
    "        actions = np.zeros((batch_size, self.action_dim), dtype=np.float32)\n",
    "        rewards = np.zeros((batch_size, 1), dtype=np.float32)\n",
    "        next_states = np.zeros((batch_size, *self.state_shape), dtype=np.float32)\n",
    "        dones = np.zeros((batch_size, 1), dtype=np.float32)\n",
    "\n",
    "        # Fill the numpy arrays\n",
    "        for i, (state, action, reward, next_state, done) in enumerate(batch):\n",
    "            states[i] = state\n",
    "            actions[i] = action\n",
    "            rewards[i] = reward\n",
    "            next_states[i] = next_state\n",
    "            dones[i] = done\n",
    "\n",
    "        # Convert numpy arrays to tensors\n",
    "        return (\n",
    "            torch.from_numpy(states).pin_memory().to(self.device,non_blocking=True),\n",
    "            torch.from_numpy(actions).to(self.device),\n",
    "            torch.from_numpy(rewards).to(self.device),\n",
    "            torch.from_numpy(next_states).to(self.device),\n",
    "            torch.from_numpy(dones).to(self.device)\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUNoise:\n",
    "    def __init__(self, size, mu=0., theta=0.15, sigma=0.2):\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.random.randn(len(x))\n",
    "        self.state = x + dx\n",
    "        return self.state.tolist()  # Return as a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, state_shape, action_size, learning_rate=1e-4, replay_buffer_size=10000, minibatch_size=32, tau=1e-3):\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.state_shape = state_shape\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        self.actor = Brain(state_shape, action_size).to(self.device)\n",
    "        self.critic = Brain(state_shape, 1).to(self.device)\n",
    "        self.target_actor = Brain(state_shape, action_size).to(self.device)\n",
    "        self.target_critic = Brain(state_shape, 1).to(self.device)\n",
    "        \n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=learning_rate)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=learning_rate)\n",
    "        \n",
    "        self.memory = ReplayMemory(replay_buffer_size, state_shape, action_size, self.device)\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.tau = tau\n",
    "\n",
    "    def act(self, state):\n",
    "        self.actor.eval()\n",
    "        with torch.no_grad():\n",
    "            action = self.actor(state).cpu().numpy().squeeze()\n",
    "        self.actor.train()\n",
    "        return np.clip(action, -1, 1)\n",
    "\n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        self.memory.push(state, action, reward, next_state, done)\n",
    "        \n",
    "        if len(self.memory) > self.minibatch_size:\n",
    "            experiences = self.memory.sample(self.minibatch_size)\n",
    "            self.learn(experiences, 0.99)\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # Update critic\n",
    "        with torch.no_grad():\n",
    "            next_actions = self.target_actor(next_states)\n",
    "            Q_targets_next = self.target_critic(next_states)\n",
    "            Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "        \n",
    "        Q_expected = self.critic(states)\n",
    "        critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        \n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # Update actor\n",
    "        actor_loss = -self.critic(states).mean()\n",
    "        \n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # Update target networks\n",
    "        self.soft_update(self.critic, self.target_critic, self.tau)\n",
    "        self.soft_update(self.actor, self.target_actor, self.tau)\n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(state_shape, action_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[83], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m next_state, reward, done, _, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     26\u001b[0m next_state \u001b[38;5;241m=\u001b[39m normalize_state(next_state)\n\u001b[0;32m---> 27\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[1;32m     29\u001b[0m score \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "Cell \u001b[0;32mIn[71], line 53\u001b[0m, in \u001b[0;36mAgent.step\u001b[0;34m(self, state, action, reward, next_state, done)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt_step \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;66;03m# If enough samples are available in memory, get random subset and learn\u001b[39;00m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory) \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mminibatch_size:\n\u001b[0;32m---> 53\u001b[0m         experiences \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmemory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mminibatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearn(experiences, \u001b[38;5;241m0.99\u001b[39m)\n",
      "Cell \u001b[0;32mIn[48], line 16\u001b[0m, in \u001b[0;36mReplayMemory.sample\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msample\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch_size):\n\u001b[1;32m     14\u001b[0m     experiences \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory, k\u001b[38;5;241m=\u001b[39mbatch_size)\n\u001b[0;32m---> 16\u001b[0m     states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43me\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mexperiences\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     17\u001b[0m     actions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39marray([e[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m experiences \u001b[38;5;28;01mif\u001b[39;00m e \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]))\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     18\u001b[0m     rewards \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39marray([e[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m experiences \u001b[38;5;28;01mif\u001b[39;00m e \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]))\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train_agent(agent, env, number_episodes=2000, max_timesteps_per_episode=1000, target_score=200.0):\n",
    "    scores_window = deque(maxlen=100)\n",
    "    epsilon = 1.0\n",
    "    epsilon_decay = 0.995\n",
    "    epsilon_min = 0.01\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    def normalize_state(state):\n",
    "        return (state - state.mean()) / (state.std() + 1e-8)\n",
    "\n",
    "    for episode in tqdm(range(1, number_episodes + 1), desc=\"Training Progress\"):\n",
    "        state, _ = env.reset()\n",
    "        state = normalize_state(state)\n",
    "        score = 0\n",
    "        \n",
    "        for t in range(max_timesteps_per_episode):\n",
    "            if np.random.random() < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                state_tensor = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "                with torch.no_grad():\n",
    "                    action = agent.act(state_tensor)\n",
    "            \n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            next_state = normalize_state(next_state)\n",
    "            \n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            \n",
    "            state = next_state\n",
    "            score += reward\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        scores_window.append(score)\n",
    "        epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "        \n",
    "        if episode % 100 == 0:\n",
    "            avg_score = np.mean(scores_window)\n",
    "            print(f'\\rEpisode {episode}\\tAverage Score: {avg_score:.2f}\\tEpsilon: {epsilon:.2f}')\n",
    "            torch.save(agent.actor.state_dict(), f'checkpoint_actor_{episode}.pth')\n",
    "            torch.save(agent.critic.state_dict(), f'checkpoint_critic_{episode}.pth')\n",
    "        \n",
    "        if np.mean(scores_window) >= target_score:\n",
    "            print(f'\\nEnvironment solved in {episode-100:d} episodes!\\tAverage Score: {np.mean(scores_window):.2f}')\n",
    "            torch.save(agent.actor.state_dict(), 'checkpoint_actor_final.pth')\n",
    "            torch.save(agent.critic.state_dict(), 'checkpoint_critic_final.pth')\n",
    "            return True\n",
    "    \n",
    "    print(f'\\nEnvironment not solved. Final Average Score: {np.mean(scores_window):.2f}')\n",
    "    torch.save(agent.actor.state_dict(), 'checkpoint_actor_final.pth')\n",
    "    torch.save(agent.critic.state_dict(), 'checkpoint_critic_final.pth')\n",
    "    return False\n",
    "\n",
    "train_agent(agent, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "in method 'b2RevoluteJoint___SetMotorSpeed', argument 2 of type 'float32'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[80], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(action, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m     24\u001b[0m     action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(action, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m---> 26\u001b[0m next_state, reward, done, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m    \u001b[38;5;66;03m# Normalize and convert types\u001b[39;00m\n\u001b[1;32m     28\u001b[0m next_state \u001b[38;5;241m=\u001b[39m next_state\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(next_state, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;28;01melse\u001b[39;00m next_state\n",
      "File \u001b[0;32m~/Documents/work/ADPS/adps/lib/python3.12/site-packages/gymnasium/wrappers/time_limit.py:57\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m     47\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m \n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/Documents/work/ADPS/adps/lib/python3.12/site-packages/gymnasium/wrappers/order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/work/ADPS/adps/lib/python3.12/site-packages/gymnasium/wrappers/env_checker.py:51\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/work/ADPS/adps/lib/python3.12/site-packages/gymnasium/envs/box2d/car_racing.py:549\u001b[0m, in \u001b[0;36mCarRacing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcar\u001b[38;5;241m.\u001b[39mgas(\u001b[38;5;241m0.2\u001b[39m \u001b[38;5;241m*\u001b[39m (action \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m))\n\u001b[1;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcar\u001b[38;5;241m.\u001b[39mbrake(\u001b[38;5;241m0.8\u001b[39m \u001b[38;5;241m*\u001b[39m (action \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m))\n\u001b[0;32m--> 549\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mFPS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworld\u001b[38;5;241m.\u001b[39mStep(\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m FPS, \u001b[38;5;241m6\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m30\u001b[39m, \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m30\u001b[39m)\n\u001b[1;32m    551\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m FPS\n",
      "File \u001b[0;32m~/Documents/work/ADPS/adps/lib/python3.12/site-packages/gymnasium/envs/box2d/car_dynamics.py:177\u001b[0m, in \u001b[0;36mCar.step\u001b[0;34m(self, dt)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28mdir\u001b[39m \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msign(w\u001b[38;5;241m.\u001b[39msteer \u001b[38;5;241m-\u001b[39m w\u001b[38;5;241m.\u001b[39mjoint\u001b[38;5;241m.\u001b[39mangle)\n\u001b[1;32m    176\u001b[0m val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mabs\u001b[39m(w\u001b[38;5;241m.\u001b[39msteer \u001b[38;5;241m-\u001b[39m w\u001b[38;5;241m.\u001b[39mjoint\u001b[38;5;241m.\u001b[39mangle)\n\u001b[0;32m--> 177\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmotorSpeed\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdir\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;241m50.0\u001b[39m \u001b[38;5;241m*\u001b[39m val, \u001b[38;5;241m3.0\u001b[39m)\n\u001b[1;32m    179\u001b[0m \u001b[38;5;66;03m# Position => friction_limit\u001b[39;00m\n\u001b[1;32m    180\u001b[0m grass \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: in method 'b2RevoluteJoint___SetMotorSpeed', argument 2 of type 'float32'"
     ]
    }
   ],
   "source": [
    "def normalize_state(state):\n",
    "    return (state - state.mean()) / (state.std() + 1e-8).astype(np.float32)\n",
    "\n",
    "number_episodes = 5000  # Increased from 2000\n",
    "max_timesteps_per_episode = 1000\n",
    "scores_window = deque(maxlen=100)\n",
    "\n",
    "epsilon = 1.0\n",
    "epsilon_decay = 0.995\n",
    "epsilon_min = 0.01\n",
    "\n",
    "for episode in range(1, number_episodes + 1):\n",
    "    state, _ = env.reset()\n",
    "    state = normalize_state(state).astype(np.float32)\n",
    "    agent.noise.reset()\n",
    "    score = 0\n",
    "    \n",
    "    for t in range(max_timesteps_per_episode):\n",
    "        if np.random.random() < epsilon:\n",
    "            action = env.action_space\n",
    "            \n",
    "        else:\n",
    "            action = agent.act(state)\n",
    "        if isinstance(action, list):\n",
    "            action = np.array(action, dtype=np.float32)\n",
    "            \n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "           # Normalize and convert types\n",
    "        next_state = next_state.astype(np.float32) if isinstance(next_state, np.ndarray) else next_state\n",
    "        \n",
    "        reward = np.float32(reward)\n",
    "        agent.step(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        score += reward\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    scores_window.append(score)\n",
    "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "    \n",
    "    # print(f'\\rEpisode {episode}\\tAverage Score: {np.mean(scores_window):.2f}\\tEpsilon: {epsilon:.2f}')\n",
    "    if episode % 100 == 0:\n",
    "        print(f'\\rEpisode {episode}\\tAverage Score: {np.mean(scores_window):.2f}\\tEpsilon: {epsilon:.2f}')\n",
    "        # Save checkpoints periodically\n",
    "        torch.save(agent.actor.state_dict(), f'checkpoint_actor_{episode}.pth')\n",
    "        torch.save(agent.critic.state_dict(), f'checkpoint_critic_{episode}.pth')\n",
    "    \n",
    "    if np.mean(scores_window) >= 200.0:\n",
    "        print(f'\\nEnvironment solved in {episode-100:d} episodes!\\tAverage Score: {np.mean(scores_window):.2f}')\n",
    "        torch.save(agent.actor.state_dict(), 'checkpoint_actor_final.pth')\n",
    "        torch.save(agent.critic.state_dict(), 'checkpoint_critic_final.pth')\n",
    "        break\n",
    "\n",
    "# If not solved, save the final model anyway\n",
    "if np.mean(scores_window) < 200.0:\n",
    "    print(f'\\nEnvironment not solved. Final Average Score: {np.mean(scores_window):.2f}')\n",
    "    torch.save(agent.actor.state_dict(), 'checkpoint_actor_final.pth')\n",
    "    torch.save(agent.critic.state_dict(), 'checkpoint_critic_final.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import io\n",
    "import base64\n",
    "import imageio\n",
    "from IPython.display import HTML, display\n",
    "from gymnasium.wrappers.monitoring.video_recorder import VideoRecorder\n",
    "\n",
    "def show_video_of_model(agent, env_name):\n",
    "    env = gym.make(env_name, render_mode='rgb_array')\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    frames = []\n",
    "    while not done:\n",
    "        frame = env.render()\n",
    "        frames.append(frame)\n",
    "        action = agent.act(state)\n",
    "        state, reward, done, _, _ = env.step(action.item())\n",
    "    env.close()\n",
    "    imageio.mimsave('video.mp4', frames, fps=30)\n",
    "\n",
    "show_video_of_model(agent, 'LunarLander-v2')\n",
    "\n",
    "def show_video():\n",
    "    mp4list = glob.glob('*.mp4')\n",
    "    if len(mp4list) > 0:\n",
    "        mp4 = mp4list[0]\n",
    "        video = io.open(mp4, 'r+b').read()\n",
    "        encoded = base64.b64encode(video)\n",
    "        display(HTML(data='''<video alt=\"test\" autoplay\n",
    "                loop controls style=\"height: 400px;\">\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii'))))\n",
    "    else:\n",
    "        print(\"Could not find video\")\n",
    "\n",
    "show_video()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
