{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Beta\n",
    "from torch.utils.data.sampler import BatchSampler, SubsetRandomSampler\n",
    "from utils import DrawLine\n",
    "class Args:\n",
    "    gamma = 0.99\n",
    "    action_repeat = 8\n",
    "    img_stack = 4\n",
    "    seed = 0\n",
    "    render = False\n",
    "    vis = False\n",
    "    log_interval = 10\n",
    "\n",
    "args = Args()\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "torch.manual_seed(args.seed)\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "transition = np.dtype([('s', np.float64, (args.img_stack, 96, 96)), ('a', np.float64, (3,)), ('a_logp', np.float64),('r', np.float64), ('s_', np.float64, (args.img_stack, 96, 96))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "class Env:\n",
    "    \"\"\"\n",
    "    Environment wrapper for CarRacing\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, args):\n",
    "        self.env = gym.make('CarRacing-v3')\n",
    "        self.reward_threshold = self.env.spec.reward_threshold\n",
    "        self.args = args\n",
    "\n",
    "    def reset(self):\n",
    "        self.counter = 0\n",
    "        self.av_r = self.reward_memory()\n",
    "        self.die = False\n",
    "\n",
    "        obs, _ = self.env.reset()  # Handle tuple return\n",
    "        img_rgb = obs\n",
    "        img_gray = self.rgb2gray(img_rgb)\n",
    "        self.stack = [img_gray] * self.args.img_stack\n",
    "        return np.array(self.stack)\n",
    "\n",
    "    def step(self, action):\n",
    "        total_reward = 0\n",
    "        for _ in range(self.args.action_repeat):\n",
    "            img_rgb, reward, terminated, truncated, _ = self.env.step(action)\n",
    "            die = terminated or truncated\n",
    "\n",
    "            if die:\n",
    "                reward += 100\n",
    "            if np.mean(img_rgb[:, :, 1]) > 185.0:\n",
    "                reward -= 0.05\n",
    "\n",
    "            total_reward += reward\n",
    "            done = True if self.av_r(reward) <= -0.1 else False\n",
    "\n",
    "            if done or die:\n",
    "                break\n",
    "\n",
    "        img_gray = self.rgb2gray(img_rgb)\n",
    "        self.stack.pop(0)\n",
    "        self.stack.append(img_gray)\n",
    "        assert len(self.stack) == self.args.img_stack\n",
    "        return np.array(self.stack), total_reward, done, die\n",
    "\n",
    "    def render(self, *arg):\n",
    "        self.env.render(*arg)\n",
    "\n",
    "    @staticmethod\n",
    "    def rgb2gray(rgb, norm=True):\n",
    "        if isinstance(rgb, tuple):\n",
    "            rgb = np.array(rgb, dtype=np.float64)  # Ensure consistent dtype\n",
    "\n",
    "        if not (rgb.ndim == 3 and rgb.shape[2] == 3):\n",
    "            raise ValueError(\"Input must be an RGB image with shape (height, width, 3)\")\n",
    "\n",
    "        gray = np.dot(rgb[..., :3], [0.299, 0.587, 0.114])\n",
    "        if norm:\n",
    "            gray = gray / 128. - 1.\n",
    "        return gray\n",
    "\n",
    "    @staticmethod\n",
    "    def reward_memory():\n",
    "        count = 0\n",
    "        length = 100\n",
    "        history = np.zeros(length)\n",
    "\n",
    "        def memory(reward):\n",
    "            nonlocal count\n",
    "            history[count] = reward\n",
    "            count = (count + 1) % length\n",
    "            return np.mean(history)\n",
    "\n",
    "        return memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    Actor-Critic Network for PPO\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.cnn_base = nn.Sequential(  # input shape (4, 96, 96)\n",
    "            nn.Conv2d(args.img_stack, 8, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),  # activation\n",
    "            nn.Conv2d(8, 16, kernel_size=3, stride=2),  # (8, 47, 47)\n",
    "            nn.ReLU(),  # activation\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=2),  # (16, 23, 23)\n",
    "            nn.ReLU(),  # activation\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2),  # (32, 11, 11)\n",
    "            nn.ReLU(),  # activation\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1),  # (64, 5, 5)\n",
    "            nn.ReLU(),  # activation\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1),  # (128, 3, 3)\n",
    "            nn.ReLU(),  # activation\n",
    "        )  # output shape (256, 1, 1)\n",
    "        self.v = nn.Sequential(nn.Linear(256, 100), nn.ReLU(), nn.Linear(100, 1))\n",
    "        self.fc = nn.Sequential(nn.Linear(256, 100), nn.ReLU())\n",
    "        self.alpha_head = nn.Sequential(nn.Linear(100, 3), nn.Softplus())\n",
    "        self.beta_head = nn.Sequential(nn.Linear(100, 3), nn.Softplus())\n",
    "        self.apply(self._weights_init)\n",
    "\n",
    "    @staticmethod\n",
    "    def _weights_init(m):\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            nn.init.xavier_uniform_(m.weight, gain=nn.init.calculate_gain('relu'))\n",
    "            nn.init.constant_(m.bias, 0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn_base(x)\n",
    "        x = x.view(-1, 256)\n",
    "        v = self.v(x)\n",
    "        x = self.fc(x)\n",
    "        alpha = self.alpha_head(x) + 1\n",
    "        beta = self.beta_head(x) + 1\n",
    "\n",
    "        return (alpha, beta), v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \"\"\"\n",
    "    Agent for training\n",
    "    \"\"\"\n",
    "    max_grad_norm = 0.5\n",
    "    clip_param = 0.1  # epsilon in clipped loss\n",
    "    ppo_epoch = 10\n",
    "    buffer_capacity, batch_size = 2000, 128\n",
    "\n",
    "    def __init__(self):\n",
    "        self.training_step = 0\n",
    "        self.net = Net().double().to(device)\n",
    "        self.buffer = np.empty(self.buffer_capacity, dtype=transition)\n",
    "        self.counter = 0\n",
    "\n",
    "        self.optimizer = optim.Adam(self.net.parameters(), lr=1e-3)\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.from_numpy(state).double().to(device).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            alpha, beta = self.net(state)[0]\n",
    "        dist = Beta(alpha, beta)\n",
    "        action = dist.sample()\n",
    "        a_logp = dist.log_prob(action).sum(dim=1)\n",
    "\n",
    "        action = action.squeeze().cpu().numpy()\n",
    "        a_logp = a_logp.item()\n",
    "        return action, a_logp\n",
    "\n",
    "    def save_param(self):\n",
    "        torch.save(self.net.state_dict(), 'param/ppo_net_params.pkl')\n",
    "\n",
    "    def store(self, transition):\n",
    "        self.buffer[self.counter] = transition\n",
    "        self.counter += 1\n",
    "        if self.counter == self.buffer_capacity:\n",
    "            self.counter = 0\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def update(self):\n",
    "        self.training_step += 1\n",
    "\n",
    "        s = torch.tensor(self.buffer['s'], dtype=torch.double).to(device)\n",
    "        a = torch.tensor(self.buffer['a'], dtype=torch.double).to(device)\n",
    "        r = torch.tensor(self.buffer['r'], dtype=torch.double).to(device).view(-1, 1)\n",
    "        s_ = torch.tensor(self.buffer['s_'], dtype=torch.double).to(device)\n",
    "\n",
    "        old_a_logp = torch.tensor(self.buffer['a_logp'], dtype=torch.double).to(device).view(-1, 1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            target_v = r + args.gamma * self.net(s_)[1]\n",
    "            adv = target_v - self.net(s)[1]\n",
    "\n",
    "        for _ in range(self.ppo_epoch):\n",
    "            for index in BatchSampler(SubsetRandomSampler(range(self.buffer_capacity)), self.batch_size, False):\n",
    "\n",
    "                alpha, beta = self.net(s[index])[0]\n",
    "                dist = Beta(alpha, beta)\n",
    "                a_logp = dist.log_prob(a[index]).sum(dim=1, keepdim=True)\n",
    "                ratio = torch.exp(a_logp - old_a_logp[index])\n",
    "\n",
    "                surr1 = ratio * adv[index]\n",
    "                surr2 = torch.clamp(ratio, 1.0 - self.clip_param, 1.0 + self.clip_param) * adv[index]\n",
    "                action_loss = -torch.min(surr1, surr2).mean()\n",
    "                value_loss = F.smooth_l1_loss(self.net(s[index])[1], target_v[index])\n",
    "                loss = action_loss + 2. * value_loss\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: <class 'numpy.ndarray'>, Shape: (4, 96, 96)\n"
     ]
    }
   ],
   "source": [
    "env = Env(args)\n",
    "state = env.reset()\n",
    "print(f\"Type: {type(state)}, Shape: {getattr(state, 'shape', 'N/A')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition Sample: [('s', '<f8', (4, 96, 96)), ('a', '<f8', (3,)), ('a_logp', '<f8'), ('r', '<f8'), ('s_', '<f8', (4, 96, 96))]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Transition Sample: {transition}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 0\tLast score: 107.53\tMoving average score: 1.08\n",
      "Ep 10\tLast score: -17.92\tMoving average score: 0.15\n",
      "updating\n",
      "Ep 20\tLast score: -17.86\tMoving average score: 1.38\n",
      "Ep 30\tLast score: -17.88\tMoving average score: 4.24\n",
      "updating\n",
      "Ep 40\tLast score: -17.83\tMoving average score: 4.32\n",
      "Ep 50\tLast score: -18.04\tMoving average score: 4.31\n",
      "updating\n",
      "Ep 60\tLast score: -9.35\tMoving average score: 5.61\n",
      "Ep 70\tLast score: -17.96\tMoving average score: 3.63\n",
      "updating\n",
      "Ep 80\tLast score: -17.93\tMoving average score: 5.65\n",
      "updating\n",
      "Ep 90\tLast score: -17.98\tMoving average score: 5.69\n",
      "Ep 100\tLast score: 12.24\tMoving average score: 4.84\n",
      "updating\n",
      "Ep 110\tLast score: -18.03\tMoving average score: 4.31\n",
      "Ep 120\tLast score: -17.99\tMoving average score: 6.43\n",
      "updating\n",
      "Ep 130\tLast score: 111.52\tMoving average score: 8.93\n",
      "Ep 140\tLast score: -18.04\tMoving average score: 6.89\n",
      "updating\n",
      "Ep 150\tLast score: -17.85\tMoving average score: 6.91\n",
      "Ep 160\tLast score: -17.95\tMoving average score: 4.97\n",
      "updating\n",
      "Ep 170\tLast score: 85.53\tMoving average score: 5.46\n",
      "Ep 180\tLast score: -18.08\tMoving average score: 4.96\n",
      "updating\n",
      "Ep 190\tLast score: 15.03\tMoving average score: 7.21\n",
      "Ep 200\tLast score: -18.09\tMoving average score: 7.36\n",
      "Ep 210\tLast score: 17.26\tMoving average score: 6.74\n",
      "updating\n",
      "Ep 220\tLast score: 32.17\tMoving average score: 6.40\n",
      "Ep 230\tLast score: 9.77\tMoving average score: 6.04\n",
      "updating\n",
      "Ep 240\tLast score: -18.06\tMoving average score: 5.68\n",
      "Ep 250\tLast score: 16.02\tMoving average score: 6.41\n",
      "Ep 260\tLast score: -18.08\tMoving average score: 6.58\n",
      "updating\n",
      "Ep 270\tLast score: 10.46\tMoving average score: 6.02\n",
      "Ep 280\tLast score: -17.97\tMoving average score: 6.43\n",
      "Ep 290\tLast score: 11.64\tMoving average score: 6.67\n",
      "updating\n",
      "Ep 300\tLast score: -18.06\tMoving average score: 6.58\n",
      "Ep 310\tLast score: 19.01\tMoving average score: 7.41\n",
      "Ep 320\tLast score: 13.42\tMoving average score: 8.54\n",
      "updating\n",
      "Ep 330\tLast score: 28.78\tMoving average score: 8.94\n",
      "Ep 340\tLast score: 32.84\tMoving average score: 10.12\n",
      "Ep 350\tLast score: 10.67\tMoving average score: 10.74\n",
      "updating\n",
      "Ep 360\tLast score: 25.18\tMoving average score: 11.00\n",
      "Ep 370\tLast score: 22.64\tMoving average score: 11.69\n",
      "Ep 380\tLast score: 34.73\tMoving average score: 12.43\n",
      "Ep 390\tLast score: 18.62\tMoving average score: 12.95\n",
      "updating\n",
      "Ep 400\tLast score: 45.63\tMoving average score: 13.34\n",
      "Ep 410\tLast score: 18.68\tMoving average score: 13.93\n",
      "Ep 420\tLast score: 32.49\tMoving average score: 13.63\n",
      "updating\n",
      "Ep 430\tLast score: 54.14\tMoving average score: 14.67\n",
      "Ep 440\tLast score: 27.13\tMoving average score: 15.33\n",
      "Ep 450\tLast score: -18.00\tMoving average score: 14.22\n",
      "updating\n",
      "Ep 460\tLast score: 15.76\tMoving average score: 14.48\n",
      "Ep 470\tLast score: 21.18\tMoving average score: 14.97\n",
      "Ep 480\tLast score: -18.00\tMoving average score: 14.31\n",
      "Ep 490\tLast score: 13.75\tMoving average score: 15.29\n",
      "updating\n",
      "Ep 500\tLast score: 27.62\tMoving average score: 14.79\n",
      "Ep 510\tLast score: 32.65\tMoving average score: 16.91\n",
      "Ep 520\tLast score: 9.91\tMoving average score: 16.54\n",
      "updating\n",
      "Ep 530\tLast score: 13.91\tMoving average score: 16.18\n",
      "Ep 540\tLast score: -31.69\tMoving average score: 16.51\n",
      "Ep 550\tLast score: -17.92\tMoving average score: 17.05\n",
      "Ep 560\tLast score: 186.21\tMoving average score: 18.44\n",
      "updating\n",
      "Ep 570\tLast score: 17.17\tMoving average score: 21.78\n",
      "Ep 580\tLast score: 10.33\tMoving average score: 22.04\n",
      "Ep 590\tLast score: 8.62\tMoving average score: 24.72\n",
      "updating\n",
      "Ep 600\tLast score: 22.21\tMoving average score: 26.30\n",
      "Ep 610\tLast score: 56.02\tMoving average score: 26.32\n",
      "Ep 620\tLast score: 46.41\tMoving average score: 27.16\n",
      "updating\n",
      "Ep 630\tLast score: 27.92\tMoving average score: 26.02\n",
      "Ep 640\tLast score: 25.31\tMoving average score: 25.36\n",
      "Ep 650\tLast score: 25.06\tMoving average score: 25.01\n",
      "updating\n",
      "Ep 660\tLast score: 26.73\tMoving average score: 26.48\n",
      "Ep 670\tLast score: 26.88\tMoving average score: 26.97\n",
      "Ep 680\tLast score: 27.61\tMoving average score: 27.31\n",
      "updating\n",
      "Ep 690\tLast score: 25.79\tMoving average score: 28.29\n",
      "Ep 700\tLast score: 34.45\tMoving average score: 29.14\n",
      "Ep 710\tLast score: 41.41\tMoving average score: 31.20\n",
      "Ep 720\tLast score: 20.26\tMoving average score: 31.36\n",
      "updating\n",
      "Ep 730\tLast score: 56.05\tMoving average score: 31.38\n",
      "Ep 740\tLast score: 12.97\tMoving average score: 32.28\n",
      "Ep 750\tLast score: -17.97\tMoving average score: 33.09\n",
      "updating\n",
      "Ep 760\tLast score: 31.32\tMoving average score: 33.87\n",
      "Ep 770\tLast score: 27.32\tMoving average score: 33.68\n",
      "updating\n",
      "Ep 780\tLast score: 25.44\tMoving average score: 35.23\n",
      "Ep 790\tLast score: 8.19\tMoving average score: 34.90\n",
      "Ep 800\tLast score: 40.59\tMoving average score: 38.17\n",
      "updating\n",
      "Ep 810\tLast score: 184.78\tMoving average score: 42.34\n",
      "Ep 820\tLast score: 216.29\tMoving average score: 44.47\n",
      "Ep 830\tLast score: 38.16\tMoving average score: 43.52\n",
      "updating\n",
      "Ep 840\tLast score: 32.16\tMoving average score: 43.38\n",
      "Ep 850\tLast score: 23.43\tMoving average score: 43.69\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    agent = Agent()\n",
    "    env = Env(args)\n",
    "    # img_rgb = env.reset()\n",
    "    # print(type(img_rgb))\n",
    "    # print(img_rgb)\n",
    "    if args.vis:\n",
    "        draw_reward = DrawLine(env=\"car\", title=\"PPO\", xlabel=\"Episode\", ylabel=\"Moving averaged episode reward\")\n",
    "\n",
    "    training_records = []\n",
    "    running_score = 0\n",
    "    state = env.reset()\n",
    "    for i_ep in range(100000):\n",
    "        score = 0\n",
    "        state = env.reset()\n",
    "\n",
    "        for t in range(1000):\n",
    "            action, a_logp = agent.select_action(state)\n",
    "            state_, reward, done, die = env.step(action * np.array([2., 1., 1.]) + np.array([-1., 0., 0.]))\n",
    "            if args.render:\n",
    "                env.render()\n",
    "            if agent.store((state, action, a_logp, reward, state_)):\n",
    "                print('updating')\n",
    "                agent.update()\n",
    "            score += reward\n",
    "            state = state_\n",
    "            if done or die:\n",
    "                break\n",
    "        running_score = running_score * 0.99 + score * 0.01\n",
    "\n",
    "        if i_ep % args.log_interval == 0:\n",
    "            if args.vis:\n",
    "                draw_reward(xdata=i_ep, ydata=running_score)\n",
    "            print('Ep {}\\tLast score: {:.2f}\\tMoving average score: {:.2f}'.format(i_ep, score, running_score))\n",
    "            agent.save_param()\n",
    "        if running_score > env.reward_threshold:\n",
    "            print(\"Solved! Running reward is now {} and the last episode runs to {}!\".format(running_score, score))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
